for (i in 1:NBS) {
bssamp[i] <- median(sample(data, replace = T))
}
conf = quantile(bssamp,c(.025,.5,.975))
return(list(bssamp,conf))
}
l <- fun.boot(rnorm(100)),NBS)
fun.boot <- function(data,NBS) {
bssamp <- numeric(NBS)
#collecting iterations
for (i in 1:NBS) {
bssamp[i] <- median(sample(data, replace = T))
}
conf = quantile(bssamp,c(.025,.5,.975))
return(list(bssamp,conf))
}
l <- fun.boot(c(1,2,3,4,5),NBS)
hist(as.numeric(unlist(l[1])),breaks=100,xlab="Bootstrapped Medians of rnorm dist")
NBS<-2000
fun.boot <- function(data,NBS) {
bssamp <- numeric(NBS)
#collecting iterations
for (i in 1:NBS) {
bssamp[i] <- median(sample(data, replace = T))
}
conf = quantile(bssamp,c(.025,.5,.975))
return(list(bssamp,conf))
}
l <- fun.boot(rnorm(100),NBS)
hist(as.numeric(unlist(l[1])),breaks=100,xlab="Bootstrapped Medians of rnorm dist")
NBS<-2000
fun.boot <- function(data,NBS) {
bssamp <- numeric(NBS)
#collecting iterations
for (i in 1:NBS) {
bssamp[i] <- median(sample(data, replace = T))
}
conf = quantile(bssamp,c(.025,.5,.975))
return(list(bssamp,conf))
}
l <- fun.boot(rnorm(100),NBS)
hist(as.numeric(unlist(l[1])),breaks=100,xlab="Bootstrapped Medians of rnorm dist")
bootmed = function(x, NBS) {
z = x[!is.na(x)] #missing part->out
n = length(z)    #how many obs?
bs = sapply(1:NBS, function(i)
median ( sample(z,n, replace=TRUE)) )
confint = quantile (x = bs, probs = c(.025,  .975))
return( list(conf.int = confint, mean = mean(bs) , bs=bs))
}
# Some examples
n.sample = 100
mylambda = 200
x1 = rpois(n.sample, lambda = mylambda)
b1 =bootmed(x1,1000)
print(b1$conf.int)
hist(b1$bs,breaks = 20, main  = 'from bootstrap: clearly not Normal')
#compare to CLT results
x1.median = median(x1)
x1.sd = sd(x1)
ifnormal.confint = x1.median + c(-1,+1) * 1.96*x1.sd/sqrt(n.sample)
cat(ifnormal.confint)
b1.normal = rnorm(1000, mean = median(x1), sd=x1.sd/sqrt(n.sample))
hist(b1.normal,main='if sampling dist of median was normal' , 20)
x1
?rpois
hist(X1)
hist(x1)
?beta
?gamma
rootk = function(number, k , start = NULL)
{
if ( sum(number<0) >0 )
stop("'number' has negative elements!")
if (is.null(start) )
start  = rep(1, length(number)) # starting point
# call optim
op = optim(par = start, fn = objf , number = number , k=k , method="BFGS")
# check if it is working
if ( op$convergence != 0 )
{
print(op)
print(' *********************** ')
print("convergence did not happen!")
}
r = op$par
return(   abs(r) )
}
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
round( rootk (1:10 , 1) , 4)
objf = function(r, number , k)
{
# how good a candidate is r for root k of number
measure = sum( (number - r^k )^2 )
return (measure)
}
rootk = function(number, k , start = NULL)
{
if ( sum(number<0) >0 )
stop("'number' has negative elements!")
if (is.null(start) )
start  = rep(1, length(number)) # starting point
# call optim
op = optim(par = start, fn = objf , number = number , k=k , method="BFGS")
# check if it is working
if ( op$convergence != 0 )
{
print(op)
print(' *********************** ')
print("convergence did not happen!")
}
r = op$par
return(   abs(r) )
}
rm(list=ls())       # clear objects in memory
library(ri)         # load the RI package
set.seed(1234567)   # random number seed, so that results are reproducible
# input full schedule of potential outcomes
Y0 <- c(10,15,20,20,10,15,15)
Y1 <- c(15,15,30,15,20,15,30)
Ys <- data.frame(Y0,Y1)
colMeans(Ys)
Z  <- c(rep(1,2),rep(0,5))
Z
rep(1,2)
sample(Z)
sample(Z)
sample(Z)
sample(Z)
sample(Z)
genperms(Z)
install.packages("ri")
perms <- genperms(Z)
library(ri)
perms <- genperms(Z)
genperms(Z)
cat(ncol(perms)," = number of permutations")
probs <- genprobexact(Z,blockvar=NULL)  # inputs imply equal-probability assignment
cat(ncol(perms)," = number of permutations")
probs <- genprobexact(Z,blockvar=NULL)  # inputs imply equal-probability assignment
#clear workspace
rm( list = ls() )
setwd("/Users//ceccarelli/Downloads/")
#Load relevant packages
library(ggplot2)
library(car)
library(psych)
library(gmodels)
library(MASS)
library(plyr)
#read in data
df<- read.csv("BLR 20th July.csv", header = TRUE)
View(df)
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
gdp.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
table(potential_outcomes.data)
View(potential_outcomes.data)
View(potential_outcomes.data)
#create shorthand reference
p=potential_outcomes.data
View(potential_outcomes.data)
sum(p$Yi.1.)
sum(p$Yi.1.)/nrow(p)
eYo<-sum(p$Yi.0.)/nrow(p)
eYo<-sum(p$Yi.0.)/nrow(p)
ate<-sum(p$Yi.0-p$Yi.1.)
ate
ate<-sum(p$Yi.0)-sum(p$Yi.1.)
ate<-sum(p$Yi.0)-sum(p$Yi.1.)
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#create shorthand reference
p<-potential_outcomes.data
##View(potential_outcomes.data)
eYi<-sum(p$Yi.1.)/nrow(p)
eYo<-sum(p$Yi.0.)/nrow(p)
ate<-sum(p$Yi.0)-sum(p$Yi.1.)
ate<-sum(p$Yi.0-p$Yi.1.)/nrow(p)
(eYo - eYi) == ate
p
p.subset <- p[,c("Yi.0","Yi.1")]
p.subset <- p[c("Yi.0","Yi.1"),]
View(p.subset)
p.subset <- p[,c("Yi.0","Yi.1")]
p.subset <- p[,c('Yi.0','Yi.1')]
p.subset <- p[,c(p$Yi.0,p$Yi.1)]
p.subset <- p[c("Yi.0","Yi.1.")]
p.subset <- p[c(1:2)]
View(p.subset)
p.subset <- p[c(2:3)]
View(p.subset)
table(p.subset)
p.subset <- p[c(2:3)]
mat<-table(p.subset)
mat
num_obs = mat[,1]+mat[,2]+mat[,3]
num_obs
num_obs
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
num_obs
mat
num_obs = mat[,1]+mat[,2]+mat[,3]
num_obs
mat.percent <- mat / num_obs
mat.percent
rm( list = ls() )
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#create shorthand reference
p<-potential_outcomes.data
##View(potential_outcomes.data)
#Define E[Yi(1)]
eYi<-sum(p$Yi.1.)/nrow(p)
#Define E[Yi(0)]
eYo<-sum(p$Yi.0.)/nrow(p)
#Define  E[Yi(0) - Yi(1)]
ate<-sum(p$Yi.0-p$Yi.1.)/nrow(p)
#Test equivalence between E[Yi(0)] - E[Yi(1)] = E[Yi(0) - Yi(1)]
(eYo - eYi) == ate
#set up working envrionment
p.subset <- p[c(2:3)]
#create matrix to count observations by variable pairs
mat<-table(p.subset)
#print matrix
mat
mat.jfd
#count number of obs in matrix
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
#calc percentage of subjects in each cell (joint freq distribution)
mat.jfd <- mat / num_obs
#print new matrix
mat.jfd
mat
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
mat.jfd <- mat / num_obs
mat.jfd
mat
num_obs
mat.dist.y.i = mat.jfd[,1]+mat.jdf[,2]+mat.jdf[,3])
mat.dist.y.i
mat.dist.y.i <- mat.jfd[,1]+mat.jdf[,2]+mat.jdf[,3]
mat.jfd
mat.dist.y.i <- mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
mat.dist.y.i
mat.dist.yi.0 <- mat.jfd[1,]+mat.jfd[1,]+mat.jfd[1,]
mat.dist.yi.0
mat.dist.yi.0 <- mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]
mat.dist.yi.0
mat.jfd_a <- cbind(mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
View(mat.jfd_a)
mat.jfd_a <- cbind(mat.jfd, mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
View(mat.jfd_a)
mat.jfd <- cbind(mat.jfd, mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
View(mat.jfd)
```{r}
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#create shorthand reference
p<-potential_outcomes.data
##View(potential_outcomes.data)
#Define E[Yi(1)]
eYi<-sum(p$Yi.1.)/nrow(p)
#Define E[Yi(0)]
eYo<-sum(p$Yi.0.)/nrow(p)
#Define  E[Yi(0) - Yi(1)]
ate<-sum(p$Yi.0-p$Yi.1.)/nrow(p)
#Test equivalence between E[Yi(0)] - E[Yi(1)] = E[Yi(0) - Yi(1)]
(eYo - eYi) == ate
```
## FE 2.3
a.
```{r}
#set up working envrionment
p.subset <- p[c(2:3)]
#create matrix to count observations by variable pairs
mat<-table(p.subset)
#print matrix
mat
```
b.
```{r}
#count number of obs in matrix
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
#calc percentage of subjects in each cell (joint freq distribution)
mat.jfd <- mat / num_obs
#print new matrix
mat.jfd
```
c.
```{r}
#marginal distribution of Yi(1)
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
#add as row in matrix
mat.jfd <- rbind(mat.jfd, mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
```
d.
```{r}
#marginal distribution of Yi(0)
mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]+mat.jfd[4,]
#add as column in matrix
mat.jfd <- cbind(mat.jfd, mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]+mat.jfd[4,])
```
e.
View(mat.jfd)
```{r}
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#create shorthand reference
p<-potential_outcomes.data
##View(potential_outcomes.data)
#Define E[Yi(1)]
eYi<-sum(p$Yi.1.)/nrow(p)
#Define E[Yi(0)]
eYo<-sum(p$Yi.0.)/nrow(p)
#Define  E[Yi(0) - Yi(1)]
ate<-sum(p$Yi.0-p$Yi.1.)/nrow(p)
#Test equivalence between E[Yi(0)] - E[Yi(1)] = E[Yi(0) - Yi(1)]
(eYo - eYi) == ate
```
## FE 2.3
a.
```{r}
#set up working envrionment
p.subset <- p[c(2:3)]
#create matrix to count observations by variable pairs
mat<-table(p.subset)
#print matrix
mat
```
b.
```{r}
#count number of obs in matrix
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
#calc percentage of subjects in each cell (joint freq distribution)
mat.jfd <- mat / num_obs
#print new matrix
mat.jfd
```
c.
```{r}
#marginal distribution of Yi(1)
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
#add as row in matrix
mat.jfd <- rbind(mat.jfd, mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
```
## FE 2.2
```{r}
#set up working envrionment
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
#clear variables
rm( list = ls() )
#read in tabular data
potential_outcomes.data <- read.csv("GerberGreenBook_Chapter2_Table_2_1.csv", sep=",", header = TRUE)
#create shorthand reference
p<-potential_outcomes.data
##View(potential_outcomes.data)
#Define E[Yi(1)]
eYi<-sum(p$Yi.1.)/nrow(p)
#Define E[Yi(0)]
eYo<-sum(p$Yi.0.)/nrow(p)
#Define  E[Yi(0) - Yi(1)]
ate<-sum(p$Yi.0-p$Yi.1.)/nrow(p)
#Test equivalence between E[Yi(0)] - E[Yi(1)] = E[Yi(0) - Yi(1)]
(eYo - eYi) == ate
```
## FE 2.3
a.
```{r}
#set up working envrionment
p.subset <- p[c(2:3)]
#create matrix to count observations by variable pairs
mat<-table(p.subset)
#print matrix
mat
```
b.
```{r}
#count number of obs in matrix
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
#calc percentage of subjects in each cell (joint freq distribution)
mat.jfd <- mat / num_obs
#print new matrix
mat.jfd
```
num_obs = sum(mat[,1]+mat[,2]+mat[,3])
#calc percentage of subjects in each cell (joint freq distribution)
mat.jfd <- mat / num_obs
#print new matrix
mat.jfd
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]
mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]+mat.jfd[4,]
mat.jfd <- rbind(mat.jfd, mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,])
mat.jfd
rownames(mat.jfd)
dimnames(mat.jfd)
rowname[mat.jfd[4,]]
rownames[mat.jfd[4,]]
rownames[mat.jfd[3,]]
rownames(mat.jfd)
rownames(mat.jfd)[4]
rownames(mat.jfd)[4]<-Yi.1
rownames(mat.jfd)[4]<-"Yi.1""
aa
clear
q
rownames(mat.jfd)[4]<-"Yi.1"
rownames(mat.jfd)[4]<-"Yi.1"
rownames(mat.jfd)
mat.jfd
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]+mat.jfd[,4]
mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]+mat.jfd[4,]
mat.jfd
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
mat.jfd[,1]+mat.jfd[,2]
mat.jfd[,1]+mat.jfd[,2]
mat.jfd[,1]
mat.jfd[1,]
mat.jfd[1,]+mat.jfd[2,]+mat.jfd[3,]
mat.jfd[,1]
+mat.jfd[,3]
mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3]
mat.jfd <- cbind(mat.jfd, mat.jfd[,1]+mat.jfd[,2]+mat.jfd[,3])
mat.jfd
colname(mat.jfd)[4]<-"Yi.0"
mat.jfd
mat.jfd
colnames(mat.jfd)
colnames(mat.jfd)[4]
colnames(mat.jfd)[3]
colnames(mat.jfd)[4]<-"Yi.0"
mat.jfd
mat.jfd*mat.jfd
0.1428571*0.1428571
0.1428571*0.1428571
0.2857143* 0.2857143
visualacuity.data <- read.csv("VisualAcuity.csv", sep=",", header = TRUE)
v<-visualacuity.data
View(v)
v$treatment.effect <- v$Yi.1.-v$Yi.0.
v$treatment.effect
View(v)
hist(v$treatment.effect)
sum(v$treatment.effect) / nrow(v)
avg(v$treatment.effect)
ave(v$treatment.effect)
sum(v$Yi.1.)-sum(v$Yi.0.)
sum(v$treatment.effect) / nrow(v) == sum(v$Yi.1.)-sum(v$Yi.0.)
sum(v$Yi.1.)/nrow(v)-sum(v$Yi.0.)/nrow(v)
sum(v$treatment.effect) / nrow(v)
nrow(v)
odd_indexes<-seq(1,v,2)
rows<-nrow(v)
even_indexes<-seq(2,rows,2)
odd_indexes<-seq(1,rows,2)
v.odd <- v
View(v.odd)
v.odd[odd_indexes]
v.odd[odd_indexes,]
v.odd <- v[1:3]
v.odd
v.odd[odd_indexes,2]
v.odd[odd_indexes,2]<-NA
v.odd
v.odd[odd_indexes,3]<-NA
v.odd
v.odd <- v[1:3]
v.odd <- v[1:3]
#assign odd children to treatment by setting their hypothetical control value to NA
v.odd[odd_indexes,2]<-NA
#assign even children to control by setting their hypothetical treatment value to NA
v.odd[even_indexes,3]<-NA
v.odd
mean(v.odd$Yi.1.,na.rm = TRUE)
mean(v.odd$Yi.0.,na.rm = TRUE)
v.odd.ate <-  mean(v.odd$Yi.1.,na.rm = TRUE) -  mean(v.odd$Yi.0.,na.rm = TRUE)
v.odd.ate
-0.06-(-.04)
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
visualacuity.data <- read.csv("VisualAcuity.csv", sep=",", header = TRUE)
visualacuity.data <- read.csv("VisualAcuity.csv", sep=",", header = TRUE)
setwd('/Users/ceccarelli/MIDS/DATASCI_W241/Async Material and Sample Files/Chapter 2/')
