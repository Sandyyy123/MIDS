---
title: "Problem Set #3 - DATASCI W241"
author: "Greg Ceccarelli"
date: "October 21, 2015"
output: pdf_document
---

## Problem 1.

```{r, message=F, warning=F}
library(dplyr)
library(stargazer)
library(memisc)

setwd("/Users/ceccarelli/MIDS/DATASCI_W241/Assignments/Problem Set 3/")
#clear variables
rm( list = ls() )
#read in tabular data
d <- read.csv("broockman_green_anon_pooled_fb_users_only.csv", sep=",", header = TRUE)

##Begin Problem 1 A##
#create new data frame only containing Study 1 data
d.s1 <- filter(d, grepl("Study 1",cluster)) %>%
  na.omit()

#simple plot 
plot(x = d.s1$treat_ad, y=d.s1$name_recall)

#regress treatment against name recall
mod1_summary <- summary(lm(name_recall ~ treat_ad,d.s1))

#review line slope
abline(lm(name_recall ~ treat_ad,d))

#pull out data for confidence interval creation

ate <- mod1_summary$coefficients[2,1]
se <- mod1_summary$coefficients[2,2]

##End Problem 1 a. 
ci <- c(ate-1.96 *se, ate+1.96*se)

##Begin Problem 1 c.

#initialize clustered standard errors function
cl <- function(fm, cluster){
  require(sandwich, quietly = TRUE)
	require(lmtest, quietly = TRUE)
	M <- length(unique(cluster))
	N <- length(cluster)
	K <- fm$rank
	dfc <- (M/(M-1))*((N-1)/(N-K))
	uj <- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));
	vcovCL <- dfc*sandwich(fm, meat=crossprod(uj)/N)
	coeftest(fm, vcovCL)
}

#make sure to set cluster variable as a factor
d.s1$cluster <- factor(as.character(d.s1$cluster))

#run standard errors
c.se <- cl(lm(name_recall ~ treat_ad,d.s1), d.s1$cluster)

c.se.ate <- c.se[2,1]
c.se.se <- c.se[2,2]

##End Problem 1 c. 
c.se.ci <- c(c.se.ate -1.96 *c.se.se, c.se.ate+1.96*c.se.se)

#stargazer(cl(lm(name_recall ~ treat_ad,d.s1), d.s1$cluster), lm(name_recall ~ treat_ad,d.s1), type = "text")

#create new data frame only containing Study 2 data and removing NAs
d.s2 <- filter(d, grepl("Study 2",cluster)) %>%
  na.omit()

d.s2$cluster <- factor(as.character(d.s2$cluster))


#run standard errors
c2.se <- cl(lm(name_recall ~ treat_ad, d.s2), d.s2$cluster)

c2.se.ate <- c2.se[2,1]
c2.se.se <- c2.se[2,2]


##End Problem 1 d. 
c2.se.ci <- c(c2.se.ate -1.96 *c2.se.se, c2.se.ate+1.96*c2.se.se)

##Begin Problem 1 e.

#create new data frame only containing Study 2 data and removing NAs
d.all <- d %>%
  na.omit()

d.all <- mutate(d.all, cluster =factor(as.character(d.all$cluster)))

#run standard errors
c3.se <- cl(lm(name_recall ~ treat_ad, d.all), d.all$cluster)

##End Problem 1 e. 
c3.se.ate <- c3.se[2,1]
c3.se.pvalue <- c3.se[2,4]


##Begin Problem 1 f.

d.all <- mutate(d.all, dummy= factor(as.character(ifelse(studyno==1,0,ifelse(studyno==2,1,FALSE)))))
#filter(d.all, dummy ==5)

#d.all <- mutate(d.all, cluster =factor(as.character(d.all$cluster)))

c4.se <- cl(lm(name_recall ~ treat_ad+dummy, d.all), d.all$cluster)

c4.se

##End Problem 1 e. 
c4.se.ate <- c4.se[2,1]
c4.se.pvalue <- c4.se[2,4]

##glimpse(d.all)


```


a. Using regression without clustered standard errors (that is, ignoring the clustered assignment), compute a confidence interval for the effect of the ad on candidate name recognition in Study 1 only (the dependent variable is “name_recall”).

```{r}
ci
```


b. What are the clusters in Broockman and Green’s study? Why might taking clustering into account increase the standard errors?

In Study 1, 32,029 voters who were then assigned to 1,220 clusters across 18 age range, the 34 towns in the candidates’ district, and 2 genders. 

Clustering can increase standard errors when cluster-level noise is large or the number of clusters are small (and noise is large)


c. Now repeat part (a), but taking clustering into account. That is, compute a confidence interval for the effect of the ad on candidate name recognition in Study 1, but now correctly accounting for the clustered nature of the treatment assignment.

```{r}
c.se.ci
```

d. Repeat part (c), but now for Study 2 only.

```{r}
c2.se.ci
```

e. Repeat part (c), but using the entire sample from both studies. Do not take into account which study the data is from (more on this in a moment), but just pool the data and run one omnibus regression. What is the treatment effect estimate and associated p-value?

```{r}
cat(" ATE = ", c3.se.ate, " pvalue = ", c3.se.pvalue)
```


f. Now, repeat part (e) but include a dummy variable (a 0/1 binary variable) for whether the data are from Study 1 or Study 2. What is the treatment effect estimate and associated p-value?


```{r}
cat(" ATE = ", c4.se.ate, " pvalue = ", c4.se.pvalue)
```

G. Why did the results from parts (e) and (f) differ? Which result is biased, and why? (Hint: see pages 75-76 of Gerber and Green, with more detailed discussion optionally available on pages 116-121.)

They differ because we pooled the results of the study together. The pooled result (question e) is biased because the probability of being assigned to either study 1 or 2 is not exactly equal (i.e it's different).


h. Skim this Facebook case study and consider two claims they make reprinted below. Why might their results differ from Broockman and Green’s? Please be specific and provide examples.

In the facebook case study they use geographic location and user targeting. Additionally it is not clear that they actually employed any randomization. For example, they specifically state: "The agency relied on Facebook’s Location Targeting to reach people in two of the most populated counties in Florida, Dade and Broward, which have a combined population of 4.2 million." Accordingly, it's very likely their effect (19% difference in voting and 17% in no voting) claims are biased because its very likely that people in the ad-targeted counties were likely to vote a certain way regardless of the facebook ad saturation. This is very different than the Broockman and Green study which employed block and clustered randomization which allowed them to measure an unbiased treamtment effect.The facebook case study appears to be making a dubious causal claim based on their methodology or lackthereof. 


## Problem 2.

a. In Column 3 of Table 4A, what is the estimated ATE of providing a recycling bin on the average weight of recyclables turned in per household per week, during the six-week treatment period?  Provide a 95% confidence interval.


```{r}
p2a.ate <- 0.187
p2a.se <- .032

cat(" ATE = ", p2a.ate, " conf interval = ", c(p2a.ate - 1.96 * p2a.se, p2a.ate + 1.96*p2a.se))
```



b. In Column 3 of Table 4A, what is the estimated ATE of sending a text message reminder on the average weight of recyclables turned in per household per week?  Provide a 95% confidence interval.


```{r}
p2b.ate <- -0.024
p2b.se <- .039

cat(" ATE = ", p2b.ate, " conf interval = ", c(p2b.ate - 1.96 * p2b.se, p2b.ate + 1.96*p2b.se))
```
  

c. Which outcome measures in Table 4A show statistically significant effects (at the 5% level) of providing a recycling bin?

1. Percentage of visits turned in bag 
2. Avg. no of bins turned in per week
3. Avg. weight of recyclables turned in per week
4. Avg. market value of recyclables given per week

d. Which outcome measures in Table 4A show statistically significant effects (at the 5% level) of sending text messages?

*None*

e. Suppose that, during the two weeks before treatment, household A turns in 2kg per week more recyclables than household B does, and suppose that both households are otherwise identical (including being in the same treatment group).  From the model, how much more recycling do we predict household A to have than household B, per week, during the six weeks of treatment?   Provide a point estimate

```{r}
cat("Over six weeks there would be", .187*2*6, "more KGs of recycling turned in")
```

f. Suppose that the variable “percentage of visits turned in bag, baseline” had been left out of the regression reported in Column 1.  What would you expect to happen to the results on providing a recycling bin?  Would you expect an increase or decrease in the estimated ATE?  Would you expect an increase or decrease in the standard error?  Explain your reasoning.

Using a baseline allows us to compute a difference in difference. Without the baseline, I would expect the ate to decrease. We get additional power by using percentage of visits turn in bag, baseline as a covariate to predict the results on providing a recycling bin. We get additional power with the baseline so removing it would actually increase standard error.

g. In column 1 of Table 4A, would you say the variable “has cell phone” is a bad control?  Explain your reasoning.

No it's not. Because we're trying to measure recycling participation intensity, I don't think cell phone ownership is an outcome of recycling participation. It could have been argued in the past cell phone ownership (when it was a luxury) would be a good measure of socio economic status which thus creates a higher likelihood of being environmentally conscious but in today's day and age, cell phone usage is ubiquitous across all levels of SES.


## Problem 3.

a. What is the full experimental design for this experiment?  Tell us the dimensions, such as 2x2x3.  (Hint: the full results appear in Panel 4B.)

The full experimental design is:
You receive a bin, a bin with a stick or no bin and 
Independent of bin randomization, If you have a phone, you receive a generic SMS, a personalized SMS or no SMS
Thus it is a 3 x 3 design. 

b. In the results of Table 4B, describe the baseline category. That is, in English, how would you describe the attributes of the group of people for whom all dummy variables are equal to zero?

The baseline measure provides a pretest measurement taken prior to bin and SMS distribution. Where dummy values are equal to zero indicates that the household did not receive the treatment.

c. In column (1) of Table 4B, interpret the magnitude of the coefficient on “bin without sticker.”  What does it mean?

Households that received a bin without a sticker were 3.5 percentage points more likely to turn in recyclables compared to those who did not receive a bin.

d. In column (1) of Table 4B, which seems to have a stronger treatment effect, the recycling bin with message sticker, or the recycling bin without sticker?  How large is the magnitude of the estimated difference?

Households receiving a bin with a sticker were 4.5 percentage points more likely to turn in recyclables compared to households receiving a bin without a sticker being 3.5 percetage points more likely. Thus those receiving a stickered bin seem to have a stronger treatment effect. The magintude of the difference is 1 % with results in a 1.2% over the sample mean of 78.

e. Is this difference you just described statistically significant?  Explain which piece of information in the table allows you to answer this question.

No the difference is not statistically significant. The confidence intervals overlap and the standard errors for each coefficient allow you to answer this question. 

d. Notice that Table 4C is described as results from “fully saturated” models.  What does this mean?  Looking at the list of variables in the table, explain in what sense the model is “saturated.”

In brief, a fully saturated model is one where the right hand side of the equaiton is as flexible as possible. What this means is that there is a dummy variable set up for each condition with the intent to try and explain as much variance as possible. 


## Problem 4.


```{r, message=F, warning=F}
library(foreign)

d4 <- read.dta("karlan_data_subset_for_class.dta")

## define a function to compute 95% confidence intervals more easily (could also just grab the conf value from the summary but this is more fun)
conf.ints <- function(model,effect){
  mod <- summary(model)
  ate<-mod$coefficients[effect,1]
  se<-mod$coefficients[effect,2]
	return(c(ate-1.96 *se, ate+1.96*se))
}

##Begin Problem 4 A##


#regress bin against avg bins
p4a.mod <- summary(lm(avg_bins_treat ~ bin,d4))

#pull out data for confidence interval creation

p4a.ate <- p4a.mod$coefficients[2,1]
p4a.se <- p4a.mod$coefficients[2,2]

p4a.ci <- c(p4a.ate -1.96 * p4a.se, p4a.ate+1.96* p4a.se)

##Begin Problem 4 B##

#regress bin against avg bins using pre-treatment y as covariate
p4b.ci <- summary(lm(avg_bins_treat ~ bin + base_avg_bins_treat,d4))

## use function to compute conf
conf.ints(lm(avg_bins_treat ~ bin + base_avg_bins_treat,d4),2)

##Problem C - H code all below

```


a. For simplicity, let’s start by measuring the effect of providing a recycling bin, ignoring the SMS message treatment (and ignoring whether there was a sticker on the bin or not).  Run a regression of Y on only the bin treatment dummy, so you estimate a simple difference in means.  Provide a 95% confidence interval for the treatment effect.

```{r}
conf.ints(lm(avg_bins_treat ~ bin,d4),2)
```

b. Now add the pre-treatment value of Y as a covariate.  Provide a 95% confidence interval for the treatment effect. Explain how and why this confidence interval differs from the previous one.

The confidence interval get's smaller because we're able to compute a difference in difference using this baseline value. It increases the power and thus decreases the standard error and the confidence intervals.

```{r}
conf.ints(lm(avg_bins_treat ~ bin + base_avg_bins_treat,d4),2)
```

c. Now add the street fixed effects.  (You’ll need to use the R command factor().) Provide a 95% confidence interval for the treatment effect.  

```{r}
conf.ints(lm(avg_bins_treat ~ bin + base_avg_bins_treat + factor(as.character(street)),d4),2)
```

d. Recall that the authors described their experiment as “stratified at the street level,” which is a synonym for blocking by street.  Explain why the confidence interval with fixed effects does not differ much from the previous one.

It doesn't vary much because since street was used in the randomization procedure, it's already inherent in the results and so adding fixed effects as a covariate doesn't increase power much. 

e. Perhaps having a cell phone helps explain the level of recycling behavior. Instead of “has cell phone,” we find it easier to interpret the coefficient if we define the variable “ no cell phone.”  Give the R command to define this new variable, which equals one minus the “has cell phone” variable in the authors’ data set.  Use “no cell phone” instead of “has cell phone” in subsequent regressions with this dataset.

```{r}
d4 <- mutate(d4, nocell= factor(as.character(ifelse(havecell==1,0,ifelse(havecell==0,1,FALSE)))))
```

f. Now add “no cell phone” as a covariate to the previous regression.  Provide a 95% confidence interval for the treatment effect.  Explain why this confidence interval does not differ much from the previous one.

The reason adding no cell phone as a covariate doesn't cause the confidence interval to differ much is that it (having or not having a cell phone) is completely independent of the bin experiment.

```{r}
conf.ints(lm(avg_bins_treat ~ bin + base_avg_bins_treat + factor(as.character(street)) + nocell,d4),2)
```

g. Now let’s add in the SMS treatment.  Re-run the previous regression with “any SMS” included.  You should get the same results as in Table 4A.  Provide a 95% confidence interval for the treatment effect of the recycling bin.  Explain why this confidence interval does not differ much from the previous one.

It doesn't differ much because SMS messages ignores interaction effects (it's being treated independently).

```{r}
conf.ints(lm(avg_bins_treat ~ bin + base_avg_bins_treat + factor(as.character(street)) + nocell + sms ,d4),2)
```

h. Now reproduce the results of column 2 in Table 4B, estimating separate treatment effects for the two types of SMS treatments and the two types of recycling-bin treatments.  Provide a 95% confidence interval for the effect of the unadorned recycling bin.  Explain how your answer differs from that in part (g), and explain why you think it differs.

It differs because instead of pooling the variables (bin and sms) we've broken out each into a subtreatment. We're able to observe the lesser effect the unaddorned bin has on behavior.

```{r}
##reproducing column 2 in Table 4B
summary(lm(avg_bins_treat ~ bin_s + bin_g + sms_p + sms_g + nocell + base_avg_bins_treat +factor(as.character(street)),d4))

conf.ints(lm(avg_bins_treat ~ bin_s + bin_g + sms_p + sms_g + nocell + base_avg_bins_treat +factor(as.character(street)),d4),3)
```


## Problem 5.


```{r}
##read in data
d5 <- read.csv("ebola_rct2.csv",header = T)
```

a. Without using any covariates, answer this question with regression: What is the estimated effect of ZMapp (with standard error in parentheses) on whether someone was vomiting on day 14? What is the p-value associated with this estimate?

```{r}
d5a <- summary(lm(vomiting_day14 ~ treat_zmapp,d5))
cat(" ATE = ", d5a$coefficients[2,1], " se = ", d5a$coefficients[2,2], " p-value = ", d5a$coefficients[2,4])
conf.ints(lm(vomiting_day14 ~ treat_zmapp,d5),2)
```

b. Add covariates for vomiting on day 0 and patient temperature on day 0 to the regression from part (a) and report the ATE (with standard error). Also report the p-value.

```{r}
d5b <- summary(lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0,d5))
cat(" ATE = ", d5b$coefficients[2,1], " se = ", d5b$coefficients[2,2], " p-value = ", d5b$coefficients[2,4])
conf.ints(lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0,d5),2)
```

c. Do you prefer the estimate of the ATE reported in part (a) or part (b)? Why?

I like the estimate of ATE in part B better because we're able to add a baseline that reduces our standard error and improves our confidence interval

d. The regression from part (b) suggests that temperature is highly predictive of vomiting. Also include temperature on day 14 as a covariate in the regression from part (b) and report the ATE, the standard error, and the p-value.

```{r}
d5d <- summary(lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + temperature_day14,d5))
cat(" ATE = ", d5d$coefficients[2,1], " se = ", d5d$coefficients[2,2], " p-value = ", d5b$coefficients[2,4])
conf.ints(lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + temperature_day14, d5),2)
stargazer(lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0,d5), lm(vomiting_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + temperature_day14,d5), type = "text")
```

e. Do you prefer the estimate of the ATE reported in part (b) or part (d)? Why?

I prefer the estimate of ATE in part B rather than part D. Adding temperature_day14 is a bad control because it is actually an outcome of the treatment.

f. Now let's switch from the outcome of vomiting to the outcome of temperature, and use the same regression covariates as in part (b). Test the hypothesis that ZMapp is especially likely to reduce men’s temperatures, as compared to women’s, and describe how you did so. What do the results suggest?

The results suggest that male's actually experience 2.17 degrees higher temperature than women when being treated with zmapp.


```{r}
summary(lm(temperature_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + male,d5))
d5f <- summary(lm(temperature_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + male,d5))
cat(" ATE = ", d5f$coefficients[2,1], " se = ", d5f$coefficients[2,2], " p-value = ", d5f$coefficients[2,4])
conf.ints(lm(temperature_day14 ~ treat_zmapp + vomiting_day0 + temperature_day0 + male,d5),2)
```


g. Suppose that you had not run the regression in part (f). Instead, you speak with a colleague to learn about heterogenous treatment effects. This colleague has access to a non-anonymized version of the same dataset and reports that he had looked at heterogenous effects of the ZMapp treatment by each of 10,000 different covariates to examine whether each predicted the effectiveness of ZMapp on each of 2,000 different indicators of health, for 20,000,000 different regressions in total ... etc

I think it is very unlikely based on the above. Specifically, the analysis of subgroups (men / women) conflates the effectiveness of the drug with the genders of people who receive them. A direct experimental test would be required rather than treatment by covariate interaction analysis to distinguish whether this is really the case.

f. Now, imagine that what described in part (g) did not happen, but that you had tested this heterogeneous treatment effect, and only this heterogeneous treatment effect, of your own accord. Would you be more or less inclined to believe that the heterogeneous treatment effect really exists? Why?

If I had this theory and had conducted a direct experiment to test gender specifically (and not after the fact) then and only then could I be inclined to believe that the heterogenous treatment effect really existed (i.e. that men are more responsive to the drug than women, for example).

i. Another colleague proposes that being of African descent causes one to be more likely to get Ebola. He asks you what ideal experiment would answer this question. What would you tell him?  (Hint: refer to Chapter 1 of Mostly Harmless Econometrics.)

That's a fundamentally unanswerable question because you can't randomly manipulate people's races in isolation and you would need to do so in order to construct an examperiment to test this question. 

